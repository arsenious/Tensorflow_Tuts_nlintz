{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def init_weights(shape):\n",
    "    w = tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "    return(w)\n",
    "\n",
    "#P_ are for dropout keep probab\n",
    "#Inp -> H1 -> H2 -> Outp\n",
    "#Notice order inside matmul\n",
    "\n",
    "def model(X, w_ih1, w_h1h2, w_h2o, p_ih1, p_h1h2, p_h2o):\n",
    "    \n",
    "    #X is input\n",
    "    X = tf.nn.dropout(X, p_ih1)\n",
    "    \n",
    "    h1 = tf.nn.relu(tf.matmul(X, w_ih1))\n",
    "    h1 = tf.nn.dropout(h1, p_h1h2)\n",
    "    \n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w_h1h2))\n",
    "    h2 = tf.nn.dropout(h2, p_h2o)\n",
    "    \n",
    "    o = tf.matmul(h2, w_h2o)\n",
    "    \n",
    "    return(o)\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "tr_X = mnist.train.images\n",
    "tr_y = mnist.train.labels\n",
    "\n",
    "te_X = mnist.test.images\n",
    "te_y = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 784])\n",
    "y = tf.placeholder(\"float\", [None,  10])\n",
    "\n",
    "w_ih1  = init_weights([784, 512])\n",
    "w_h1h2 = init_weights([512, 256])\n",
    "w_h2o  = init_weights([256,  10])\n",
    "\n",
    "p_ih1  = tf.placeholder(\"float\")\n",
    "p_h1h2 = tf.placeholder(\"float\")\n",
    "p_h2o  = tf.placeholder(\"float\")\n",
    "\n",
    "y_hat = model(X, w_ih1, w_h1h2, w_h2o, p_ih1, p_h1h2, p_h2o)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat, labels=y))\n",
    "\n",
    "train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "predict_op = tf.argmax(y_hat, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter  0\n",
      "0 0.9251\n",
      "Training iter  1\n",
      "1 0.9557\n",
      "Training iter  2\n",
      "2 0.9663\n",
      "Training iter  3\n",
      "3 0.9702\n",
      "Training iter  4\n",
      "4 0.9741\n",
      "Training iter  5\n",
      "5 0.9758\n",
      "Training iter  6\n",
      "6 0.9767\n",
      "Training iter  7\n",
      "7 0.9785\n",
      "Training iter  8\n",
      "8 0.9774\n",
      "Training iter  9\n",
      "9 0.9779\n",
      "Training iter  10\n",
      "10 0.9795\n",
      "Training iter  11\n",
      "11 0.979\n",
      "Training iter  12\n",
      "12 0.9805\n",
      "Training iter  13\n",
      "13 0.98\n",
      "Training iter  14\n",
      "14 0.9795\n",
      "Training iter  15\n",
      "15 0.9813\n",
      "Training iter  16\n",
      "16 0.9798\n",
      "Training iter  17\n",
      "17 0.9806\n",
      "Training iter  18\n",
      "18 0.9806\n",
      "Training iter  19\n",
      "19 0.981\n",
      "Training iter  20\n",
      "20 0.9824\n",
      "Training iter  21\n",
      "21 0.9815\n",
      "Training iter  22\n",
      "22 0.9814\n",
      "Training iter  23\n",
      "23 0.9825\n",
      "Training iter  24\n",
      "24 0.9815\n",
      "Training iter  25\n",
      "25 0.9824\n",
      "Training iter  26\n",
      "26 0.9817\n",
      "Training iter  27\n",
      "27 0.9826\n",
      "Training iter  28\n",
      "28 0.9817\n",
      "Training iter  29\n",
      "29 0.9827\n",
      "Training iter  30\n",
      "30 0.982\n",
      "Training iter  31\n",
      "31 0.9831\n",
      "Training iter  32\n",
      "32 0.9831\n",
      "Training iter  33\n",
      "33 0.9827\n",
      "Training iter  34\n",
      "34 0.9816\n",
      "Training iter  35\n",
      "35 0.9825\n",
      "Training iter  36\n",
      "36 0.9836\n",
      "Training iter  37\n",
      "37 0.9833\n",
      "Training iter  38\n",
      "38 0.9818\n",
      "Training iter  39\n",
      "39 0.9827\n",
      "Training iter  40\n",
      "40 0.9831\n",
      "Training iter  41\n",
      "41 0.9821\n",
      "Training iter  42\n",
      "42 0.9811\n",
      "Training iter  43\n",
      "43 0.9823\n",
      "Training iter  44\n",
      "44 0.9829\n",
      "Training iter  45\n",
      "45 0.9829\n",
      "Training iter  46\n",
      "46 0.9822\n",
      "Training iter  47\n",
      "47 0.9829\n",
      "Training iter  48\n",
      "48 0.9835\n",
      "Training iter  49\n",
      "49 0.9843\n",
      "Training iter  50\n",
      "50 0.9834\n",
      "Training iter  51\n",
      "51 0.9836\n",
      "Training iter  52\n",
      "52 0.9847\n",
      "Training iter  53\n",
      "53 0.9841\n",
      "Training iter  54\n",
      "54 0.9824\n",
      "Training iter  55\n",
      "55 0.9841\n",
      "Training iter  56\n",
      "56 0.9831\n",
      "Training iter  57\n",
      "57 0.9841\n",
      "Training iter  58\n",
      "58 0.9833\n",
      "Training iter  59\n",
      "59 0.9836\n",
      "Training iter  60\n",
      "60 0.9834\n",
      "Training iter  61\n",
      "61 0.9847\n",
      "Training iter  62\n",
      "62 0.984\n",
      "Training iter  63\n",
      "63 0.9838\n",
      "Training iter  64\n",
      "64 0.9837\n",
      "Training iter  65\n",
      "65 0.9835\n",
      "Training iter  66\n",
      "66 0.9847\n",
      "Training iter  67\n",
      "67 0.9828\n",
      "Training iter  68\n",
      "68 0.9836\n",
      "Training iter  69\n",
      "69 0.9845\n",
      "Training iter  70\n",
      "70 0.9856\n",
      "Training iter  71\n",
      "71 0.9844\n",
      "Training iter  72\n",
      "72 0.9849\n",
      "Training iter  73\n",
      "73 0.9844\n",
      "Training iter  74\n",
      "74 0.985\n",
      "Training iter  75\n",
      "75 0.986\n",
      "Training iter  76\n",
      "76 0.9844\n",
      "Training iter  77\n",
      "77 0.985\n",
      "Training iter  78\n",
      "78 0.9844\n",
      "Training iter  79\n",
      "79 0.9852\n",
      "Training iter  80\n",
      "80 0.9842\n",
      "Training iter  81\n",
      "81 0.984\n",
      "Training iter  82\n",
      "82 0.985\n",
      "Training iter  83\n",
      "83 0.9839\n",
      "Training iter  84\n",
      "84 0.984\n",
      "Training iter  85\n",
      "85 0.9839\n",
      "Training iter  86\n",
      "86 0.9846\n",
      "Training iter  87\n",
      "87 0.9851\n",
      "Training iter  88\n",
      "88 0.9844\n",
      "Training iter  89\n",
      "89 0.9846\n",
      "Training iter  90\n",
      "90 0.9849\n",
      "Training iter  91\n",
      "91 0.9841\n",
      "Training iter  92\n",
      "92 0.9835\n",
      "Training iter  93\n",
      "93 0.9834\n",
      "Training iter  94\n",
      "94 0.9842\n",
      "Training iter  95\n",
      "95 0.9847\n",
      "Training iter  96\n",
      "96 0.9854\n",
      "Training iter  97\n",
      "97 0.9847\n",
      "Training iter  98\n",
      "98 0.9847\n",
      "Training iter  99\n",
      "99 0.9861\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "#Train in batches    \n",
    "    for i in range(100):\n",
    "        print(\"Training iter \", i)\n",
    "        start_list = range(0, len(tr_X), 128) #List for all start positions\n",
    "        end_list = range(128, len(tr_X)+1, 128)#List for all end positions\n",
    "        \n",
    "        for start, end in zip(start_list, end_list):\n",
    "            sess.run(train_op, feed_dict={\n",
    "                X : tr_X[start:end],\n",
    "                y : tr_y[start:end],\n",
    "                p_ih1  : 0.8,\n",
    "                p_h1h2 : 0.5,\n",
    "                p_h2o  : 0.5\n",
    "                \n",
    "            })\n",
    "        \n",
    "        \n",
    "#Testing\n",
    "\n",
    "        test_preds = sess.run(predict_op, feed_dict={\n",
    "                X : te_X,\n",
    "                y : te_y,\n",
    "                p_ih1  : 1.0,\n",
    "                p_h1h2 : 1.0,\n",
    "                p_h2o  : 1.0\n",
    "        })\n",
    "\n",
    "        test_acc = np.mean(np.argmax(te_y, axis=1) == test_preds)\n",
    "        print(i, test_acc)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
